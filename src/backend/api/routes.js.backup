/**
 * API routes for EDA application
 */

const express = require('express');
const multer = require('multer');
const { Worker } = require('worker_threads');
const path = require('path');
const fs = require('fs');
const { v4: uuidv4 } = require('uuid');

const router = express.Router();

// Configure multer for file uploads
const storage = multer.diskStorage({
  destination: function (req, file, cb) {
    cb(null, 'uploads/');
  },
  filename: function (req, file, cb) {
    // Create a unique filename with original extension
    const uniqueName = `${uuidv4()}${path.extname(file.originalname)}`;
    cb(null, uniqueName);
  }
});

const upload = multer({
  storage: storage,
  limits: { fileSize: 1024 * 1024 * 100 }, // 100MB limit
  fileFilter: (req, file, cb) => {
    // Check file extension more thoroughly
    const fileExtension = path.extname(file.originalname).toLowerCase();
    
    if (file.mimetype === 'text/csv' || 
        file.mimetype === 'application/csv' ||
        file.mimetype === 'application/vnd.ms-excel' ||
        fileExtension === '.csv') {
      cb(null, true);
    } else {
      cb(new Error('Only CSV files are allowed! Please check your file format.'), false);
    }
  }
});

// Data storage (In a production app, use a proper database)
const dataStore = {};

// Enhanced cache system for frequently accessed data with memory monitoring and LRU eviction
const dataCache = {
  sampleData: new Map(),     // Cache for dataset samples
  analysis: new Map(),       // Cache for analysis results
  visualizations: new Map(), // Cache for generated visualizations
  charts: new Map(),         // Cache for chart specifications
  metadata: new Map(),       // Cache for dataset metadata
  statistics: new Map(),     // Cache for calculated statistics
  lastCleanup: Date.now(),   // Last cleanup timestamp
  hits: { sampleData: 0, analysis: 0, visualizations: 0, charts: 0, metadata: 0, statistics: 0 },
  misses: { sampleData: 0, analysis: 0, visualizations: 0, charts: 0, metadata: 0, statistics: 0 },
  config: {
    maxCacheSizeMB: 100,     // Maximum cache size in MB (default 100MB)
    defaultTTL: 30 * 60 * 1000, // Default TTL: 30 minutes
    cleanupInterval: 5 * 60 * 1000, // Cleanup interval: 5 minutes
    maxItemsPerType: 1000,   // Maximum items per cache type
    pruneAmount: 0.2,        // Percentage to prune when cache is full (20%)
    memoryCheckInterval: 60 * 1000 // Memory check interval: 1 minute
  }
};

/**
 * LRU Cache Entry wrapper to track access time and size
 */
class CacheEntry {
  constructor(key, data, ttl = null) {
    this.key = key;
    this.data = data;
    this.created = Date.now();
    this.lastAccessed = Date.now();
    this.expires = ttl ? Date.now() + ttl : null;
    this.size = this._estimateSize(data);
    this.accessCount = 0;
  }
  
  /**
   * Estimate memory size in bytes of an object
   * @param {any} obj Object to estimate size for
   * @returns {number} Estimated size in bytes
   */
  _estimateSize(obj) {
    if (obj === null || obj === undefined) return 0;
    
    const objectList = [];
    const stack = [obj];
    let bytes = 0;
    
    // Simple size estimation to avoid circular references and excessive computation
    while (stack.length) {
      const value = stack.pop();
      
      if (typeof value === 'boolean') {
        bytes += 4;
      } else if (typeof value === 'string') {
        bytes += value.length * 2;
      } else if (typeof value === 'number') {
        bytes += 8;
      } else if (typeof value === 'object' && objectList.indexOf(value) === -1) {
        objectList.push(value);
        
        if (Array.isArray(value)) {
          // For arrays, count array overhead plus sample a few items
          bytes += 8 + (value.length > 10 ? 10 : value.length) * 8;
          
          // Sample up to 10 items for size estimation
          for (let i = 0; i < Math.min(value.length, 10); i++) {
            stack.push(value[i]);
          }
        } else {
          // For objects, count object overhead plus all properties
          bytes += 40; // Base object overhead
          
          for (const key in value) {
            if (Object.prototype.hasOwnProperty.call(value, key)) {
              bytes += key.length * 2; // Key size (2 bytes per char for string)
              stack.push(value[key]);  // Value size will be calculated in next iterations
            }
          }
        }
      }
      
      // Cap size estimation to avoid excessive computation
      if (bytes > 100 * 1024 * 1024) { // 100MB cap
        return bytes;
      }
    }
    
    return bytes;
  }
  
  /**
   * Update last accessed time
   */
  accessed() {
    this.lastAccessed = Date.now();
    this.accessCount++;
    return this.data;
  }
  
  /**
   * Check if entry has expired
   * @returns {boolean} True if expired
   */
  isExpired() {
    return this.expires !== null && this.expires < Date.now();
  }
}

/**
 * Enhanced cache manager with memory monitoring and LRU eviction
 */
const cacheManager = {
  // Track total estimated cache size
  totalSize: 0,
  
  /**
   * Initialize cache monitoring
   */
  init() {
    // Start cache monitoring
    setInterval(() => this.monitorMemoryUsage(), dataCache.config.memoryCheckInterval);
    
    // Start regular cleanup
    setInterval(() => this.cleanupExpired(), dataCache.config.cleanupInterval);
    
    // Log message confirming initialization
    console.log('Cache manager initialized with:', {
      maxSizeMB: dataCache.config.maxCacheSizeMB,
      defaultTTL: dataCache.config.defaultTTL / 1000 / 60 + ' minutes',
      cleanupInterval: dataCache.config.cleanupInterval / 1000 / 60 + ' minutes'
    });
  },
  
  /**
   * Get item from cache
   * @param {string} cacheType Type of cache
   * @param {string} key Cache key
   * @returns {any} Cached item or null
   */
  get(cacheType, key) {
    if (!dataCache[cacheType]) return null;
    
    const entry = dataCache[cacheType].get(key);
    if (!entry) {
      // Track cache miss
      dataCache.misses[cacheType] = (dataCache.misses[cacheType] || 0) + 1;
      return null;
    }
    
    // If expired, remove and return null
    if (entry.isExpired()) {
      this.invalidate(cacheType, key);
      dataCache.misses[cacheType] = (dataCache.misses[cacheType] || 0) + 1;
      return null;
    }
    
    // Track cache hit
    dataCache.hits[cacheType] = (dataCache.hits[cacheType] || 0) + 1;
    
    // Update access time and return data
    return entry.accessed();
  },
  
  /**
   * Store item in cache
   * @param {string} cacheType Type of cache
   * @param {string} key Cache key
   * @param {any} data Data to cache
   * @param {number} ttl Optional TTL in milliseconds (default from config)
   */
  set(cacheType, key, data, ttl = dataCache.config.defaultTTL) {
    if (!dataCache[cacheType]) return;
    
    // Check cache size and prune if necessary before adding new items
    this.checkCacheSize();
    
    // Remove existing entry if present to update size tracking
    if (dataCache[cacheType].has(key)) {
      const oldEntry = dataCache[cacheType].get(key);
      this.totalSize -= oldEntry.size;
      dataCache[cacheType].delete(key);
    }
    
    // Create new cache entry
    const entry = new CacheEntry(key, data, ttl);
    
    // Update total size
    this.totalSize += entry.size;
    
    // Add to cache
    dataCache[cacheType].set(key, entry);
    
    // Check if we need to evict entries due to count limit
    if (dataCache[cacheType].size > dataCache.config.maxItemsPerType) {
      this.evictLRU(cacheType, Math.floor(dataCache.config.maxItemsPerType * dataCache.config.pruneAmount));
    }
    
    return entry.size;
  },
  
  /**
   * Evict least recently used entries from a specific cache
   * @param {string} cacheType Cache type
   * @param {number} count Number of entries to evict
   */
  evictLRU(cacheType, count) {
    if (!dataCache[cacheType] || dataCache[cacheType].size === 0) return;
    
    // Get all entries and sort by last accessed time
    const entries = Array.from(dataCache[cacheType].entries());
    const sorted = entries.sort((a, b) => a[1].lastAccessed - b[1].lastAccessed);
    
    // Remove the least recently used entries
    const toRemove = Math.min(count, sorted.length);
    for (let i = 0; i < toRemove; i++) {
      const [key, entry] = sorted[i];
      this.totalSize -= entry.size;
      dataCache[cacheType].delete(key);
    }
    
    console.log(`Cache pruning: Evicted ${toRemove} items from ${cacheType} cache`);
  },
  
  /**
   * Invalidate a cache entry
   * @param {string} cacheType Type of cache
   * @param {string} key Cache key
   */
  invalidate(cacheType, key) {
    if (!dataCache[cacheType]) return;
    
    // Remove from size tracking
    const entry = dataCache[cacheType].get(key);
    if (entry) {
      this.totalSize -= entry.size;
      dataCache[cacheType].delete(key);
      return true;
    }
    
    return false;
  },
  
  /**
   * Invalidate all entries for a specific dataset
   * @param {string} dataId Dataset ID
   */
  invalidateDataset(dataId) {
    // Invalidate all cache entries related to this dataset
    Object.keys(dataCache).forEach(cacheType => {
      if (typeof dataCache[cacheType].forEach === 'function') {
        const keysToInvalidate = [];
        
        dataCache[cacheType].forEach((entry, key) => {
          // Check if key or data contains the dataId
          if (key.includes(dataId) || 
              (entry.data && JSON.stringify(entry.data).includes(dataId))) {
            keysToInvalidate.push(key);
          }
        });
        
        // Delete all the collected keys
        keysToInvalidate.forEach(key => this.invalidate(cacheType, key));
        
        if (keysToInvalidate.length > 0) {
          console.log(`Invalidated ${keysToInvalidate.length} entries from ${cacheType} cache for dataset ${dataId}`);
        }
      }
    });
  },
  
  /**
   * Clear expired cache entries
   */
  cleanupExpired() {
    const now = Date.now();
    let totalRemoved = 0;
    let totalSizeFreed = 0;
    
    // Skip cleanup if not enough time has passed since last cleanup
    if (now - dataCache.lastCleanup < dataCache.config.cleanupInterval / 2) {
      return;
    }
    
    dataCache.lastCleanup = now;
    
    // Clean up each cache type
    Object.keys(dataCache).forEach(cacheType => {
      if (cacheType === 'config' || cacheType === 'lastCleanup' || 
          cacheType === 'hits' || cacheType === 'misses') {
        return; // Skip non-cache properties
      }
      
      const cache = dataCache[cacheType];
      if (typeof cache.forEach !== 'function') return;
      
      const keysToRemove = [];
      
      // Find expired entries
      cache.forEach((entry, key) => {
        if (entry.isExpired()) {
          keysToRemove.push(key);
          totalSizeFreed += entry.size;
        }
      });
      
      // Remove expired entries
      keysToRemove.forEach(key => {
        cache.delete(key);
        totalRemoved++;
      });
    });
    
    if (totalRemoved > 0) {
      this.totalSize -= totalSizeFreed;
      console.log(`Cache cleanup: Removed ${totalRemoved} expired entries, freed ${Math.round(totalSizeFreed / 1024 / 1024)}MB`);
    }
    
    // Check overall cache size and prune if too large
    this.checkCacheSize();
    
    // Log cache statistics periodically
    this.logCacheStats();
  },
  
  /**
   * Check if cache size exceeds maximum and prune if necessary
   */
  checkCacheSize() {
    const maxSizeBytes = dataCache.config.maxCacheSizeMB * 1024 * 1024;
    
    // If cache size exceeds max, prune across all types
    if (this.totalSize > maxSizeBytes) {
      const currentSizeMB = Math.round(this.totalSize / 1024 / 1024);
      console.log(`Cache size (${currentSizeMB}MB) exceeds maximum (${dataCache.config.maxCacheSizeMB}MB), pruning...`);
      
      // Prune each cache type proportionally
      Object.keys(dataCache).forEach(cacheType => {
        if (cacheType === 'config' || cacheType === 'lastCleanup' || 
            cacheType === 'hits' || cacheType === 'misses') {
          return; // Skip non-cache properties
        }
        
        const cache = dataCache[cacheType];
        if (typeof cache.size !== 'number' || cache.size === 0) return;
        
        // Calculate how many items to evict from this cache
        const pruneCount = Math.ceil(cache.size * dataCache.config.pruneAmount);
        this.evictLRU(cacheType, pruneCount);
      });
    }
  },
  
  /**
   * Monitor memory usage and clear cache if needed
   */
  monitorMemoryUsage() {
    const memUsage = process.memoryUsage();
    const heapUsed = memUsage.heapUsed / 1024 / 1024; // MB
    const rss = memUsage.rss / 1024 / 1024; // MB
    
    // If memory usage is too high, aggressively prune cache
    const memoryThreshold = 1024; // 1GB threshold (adjust based on your server)
    if (rss > memoryThreshold) {
      console.warn(`High memory usage detected: ${Math.round(rss)}MB RSS, ${Math.round(heapUsed)}MB heap`);
      console.warn(`Aggressive cache pruning initiated`);
      
      // Clear 50% of all caches
      Object.keys(dataCache).forEach(cacheType => {
        if (cacheType === 'config' || cacheType === 'lastCleanup' || 
            cacheType === 'hits' || cacheType === 'misses') {
          return; // Skip non-cache properties
        }
        
        const cache = dataCache[cacheType];
        if (typeof cache.size !== 'number' || cache.size === 0) return;
        
        // Evict 50% of each cache
        this.evictLRU(cacheType, Math.ceil(cache.size * 0.5));
      });
      
      // Force garbage collection if available
      if (global.gc && typeof global.gc === 'function') {
        global.gc();
      }
    }
  },
  
  /**
   * Log cache statistics
   */
  logCacheStats() {
    const stats = {
      totalSizeMB: Math.round(this.totalSize / 1024 / 1024 * 100) / 100,
      entries: {}
    };
    
    // Collect stats for each cache type
    Object.keys(dataCache).forEach(cacheType => {
      if (cacheType === 'config' || cacheType === 'lastCleanup' || 
          cacheType === 'hits' || cacheType === 'misses') {
        return; // Skip non-cache properties
      }
      
      const cache = dataCache[cacheType];
      if (typeof cache.size !== 'number') return;
      
      stats.entries[cacheType] = {
        count: cache.size,
        hits: dataCache.hits[cacheType] || 0,
        misses: dataCache.misses[cacheType] || 0
      };
      
      // Calculate hit rate
      const total = stats.entries[cacheType].hits + stats.entries[cacheType].misses;
      stats.entries[cacheType].hitRate = total > 0 
        ? Math.round((stats.entries[cacheType].hits / total) * 100) 
        : 0;
    });
    
    console.log('Cache statistics:', stats);
  },
  
  /**
   * Get cache efficiency statistics
   * @returns {Object} Cache statistics
   */
  getStats() {
    const stats = {
      totalSizeMB: Math.round(this.totalSize / 1024 / 1024 * 100) / 100,
      maxSizeMB: dataCache.config.maxCacheSizeMB,
      usage: Math.round((this.totalSize / (dataCache.config.maxCacheSizeMB * 1024 * 1024)) * 100),
      lastCleanup: new Date(dataCache.lastCleanup).toISOString(),
      types: {}
    };
    
    // Collect stats for each cache type
    Object.keys(dataCache).forEach(cacheType => {
      if (cacheType === 'config' || cacheType === 'lastCleanup' || 
          cacheType === 'hits' || cacheType === 'misses') {
        return; // Skip non-cache properties
      }
      
      const cache = dataCache[cacheType];
      if (typeof cache.size !== 'number') return;
      
      stats.types[cacheType] = {
        count: cache.size,
        hits: dataCache.hits[cacheType] || 0,
        misses: dataCache.misses[cacheType] || 0,
        hitRate: 0
      };
      
      // Calculate hit rate
      const total = stats.types[cacheType].hits + stats.types[cacheType].misses;
      stats.types[cacheType].hitRate = total > 0 
        ? Math.round((stats.types[cacheType].hits / total) * 100) 
        : 0;
    });
    
    return stats;
  }
};

// Initialize cache manager
cacheManager.init();

/**
 * Upload a CSV file
 * POST /api/upload
 */
router.post('/upload', upload.single('csvFile'), (req, res) => {
  if (!req.file) {
    return res.status(400).json({ error: 'No file uploaded' });
  }

  const filePath = req.file.path;
  const dataId = uuidv4();

  // Store file info
  dataStore[dataId] = {
    id: dataId,
    filePath,
    originalName: req.file.originalname,
    status: 'processing',
    uploadDate: new Date().toISOString(),
    progress: {
      phase: 'initializing',
      percent: 0,
      message: 'Starting CSV processing'
    }
  };

  // Process file in a worker thread
  const worker = new Worker(path.join(__dirname, '../workers/csv-processor.js'), {
    workerData: { filePath, dataId }
  });

  worker.on('message', (result) => {
    // Handle different message types
    if (result.type === 'progress') {
      // Update progress information
      const { phase, progress, rowsProcessed, bytesRead, totalBytes } = result;
      
      // Format a user-friendly message based on the phase
      let message = 'Processing CSV file';
      let percent = progress || 0;
      
      switch (phase) {
        case 'reading':
          message = `Reading file: ${Math.round(percent)}%`;
          break;
        case 'headers-detected':
          message = 'Headers detected, processing data';
          break;
        case 'processing':
        case 'processing-batch':
          message = rowsProcessed ? 
            `Processing data: ${rowsProcessed.toLocaleString()} rows` : 
            'Processing data';
          break;
        case 'finalizing':
          message = 'Finalizing statistics';
          percent = 95;
          break;
        case 'complete':
          message = `Processed ${rowsProcessed.toLocaleString()} rows`;
          percent = 100;
          break;
      }
      
      // Update data store with progress
      dataStore[dataId].progress = {
        phase,
        percent,
        message,
        rowsProcessed,
        bytesRead,
        totalBytes,
        updatedAt: Date.now()
      };
      
      // Don't respond here - the client will poll for progress
      return;
    } else if (result.type === 'complete' && result.success) {
      // Update data store with processed results
      dataStore[dataId] = {
        ...dataStore[dataId],
        ...result,
        status: 'ready',
        progress: {
          phase: 'complete',
          percent: 100,
          message: `Processing complete: ${result.summary.rowCount.toLocaleString()} rows`,
          completedAt: Date.now()
        }
      };

      // Store sample data in cache for quick access
      cacheManager.set('sampleData', dataId, result.sampleData, 30 * 60 * 1000); // 30 minute TTL

      res.json({
        success: true,
        dataId,
        summary: result.summary
      });
    } else if (result.type === 'error' || !result.success) {
      dataStore[dataId].status = 'error';
      dataStore[dataId].error = result.error;
      dataStore[dataId].errorStack = result.stack;
      dataStore[dataId].progress = {
        phase: 'error',
        percent: 0,
        message: `Error: ${result.error}`,
        errorAt: Date.now()
      };

      res.status(500).json({
        error: result.error,
        dataId
      });
    }
  });

  worker.on('error', (err) => {
    dataStore[dataId].status = 'error';
    dataStore[dataId].error = err.message;
    dataStore[dataId].progress = {
      phase: 'error',
      percent: 0,
      message: `Error: ${err.message}`,
      errorAt: Date.now()
    };

    res.status(500).json({
      error: err.message,
      dataId
    });
  });
});

/**
 * Get data summary
 * GET /api/data/:dataId
 */
router.get('/data/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  res.json({
    dataId,
    status: dataStore[dataId].status,
    name: dataStore[dataId].originalName,
    summary: dataStore[dataId].summary || null,
    error: dataStore[dataId].error || null
  });
});

/**
 * Get data sample
 * GET /api/data/:dataId/sample
 */
router.get('/data/:dataId/sample', (req, res) => {
  const { dataId } = req.params;
  const limit = parseInt(req.query.limit) || 100;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (dataStore[dataId].status !== 'ready') {
    return res.status(400).json({
      error: 'Data is not ready',
      status: dataStore[dataId].status,
      progress: dataStore[dataId].progress || null
    });
  }

  console.log(`Loading sample data for dataset ${dataId}, limit: ${limit}`);
  
  // First, check if we have cached data
  const cachedSample = cacheManager.get('sampleData', dataId);
  if (cachedSample && Array.isArray(cachedSample) && cachedSample.length > 0) {
    console.log(`Using cached sample data for ${dataId}, ${cachedSample.length} items`);
    return res.json({
      dataId,
      sample: cachedSample.slice(0, limit),
      fromCache: true
    });
  }
  
  // If no valid cache, get the sample data from the data store
  const dataStoreSample = dataStore[dataId].sampleData || [];
  console.log(`Data store sample length: ${dataStoreSample.length}`);
  
  // Check if we have valid sample data
  if (dataStoreSample.length === 0) {
    console.warn(`No sample data available for dataset ${dataId}. This will cause UI issues.`);
    
    // Check if we can retrieve the file and process it again
    if (dataStore[dataId].filePath && fs.existsSync(dataStore[dataId].filePath)) {
      console.log(`Attempting to re-process file for dataset ${dataId} to generate sample data`);
      
      // Import the memory-efficient processor
      const { MemoryEfficientCSVProcessor } = require('../data/csv-parser');
      
      try {
        // Create a new processor instance
        const processor = new MemoryEfficientCSVProcessor({
          sampleSize: 1000,
          batchSize: 5000
        });
        
        // Process the file synchronously for immediate response
        const result = processor.processFile(dataStore[dataId].filePath);
        
        // Use the promise result when available
        result.then(processedData => {
          // Store the regenerated sample data in the data store
          dataStore[dataId].sampleData = processedData.sampleData;
          
          // Store in cache for future requests
          if (processedData.sampleData.length > 0) {
            cacheManager.set('sampleData', dataId, processedData.sampleData, 30 * 60 * 1000); // 30 minute TTL
            console.log(`Regenerated and stored ${processedData.sampleData.length} sample items for dataset ${dataId}`);
          }
        }).catch(err => {
          console.error(`Error re-processing file for dataset ${dataId}:`, err);
        });
        
        // For the current request, we'll still need to return an empty sample
        // since the regeneration is async, but future requests will have data
        return res.json({
          dataId,
          sample: [],
          fromCache: false,
          message: "Sample data is being regenerated. Please refresh in a few seconds."
        });
      } catch (error) {
        console.error(`Failed to re-process file for dataset ${dataId}:`, error);
      }
    }
  } else {
    // Store valid sample data in cache for future requests
    console.log(`Storing ${dataStoreSample.length} items in cache for dataset ${dataId}`);
    cacheManager.set('sampleData', dataId, dataStoreSample, 30 * 60 * 1000); // 30 minute TTL
  }
  
  res.json({
    dataId,
    sample: dataStoreSample.slice(0, limit),
    fromCache: false
  });
});

/**
 * Get processing progress
 * GET /api/data/:dataId/progress
 */
router.get('/data/:dataId/progress', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  // Return current progress information
  res.json({
    dataId,
    status: dataStore[dataId].status,
    progress: dataStore[dataId].progress || { phase: 'unknown', percent: 0 }
  });
});

/**
 * Get comprehensive progress for any operation
 * GET /api/progress/:operationId
 */
router.get('/progress/:operationId', (req, res) => {
  const { operationId } = req.params;
  const operations = {
    // Each operation type has its progress indexed by the relevant ID
    data: dataStore,
    viz: Object.values(dataStore).reduce((acc, dataset) => {
      if (dataset.vizProgress) {
        Object.entries(dataset.vizProgress).forEach(([vizId, progress]) => {
          acc[vizId] = { 
            ...progress, 
            dataId: dataset.id,
            type: 'visualization' 
          };
        });
      }
      return acc;
    }, {}),
    analysis: Object.values(dataStore).reduce((acc, dataset) => {
      if (dataset.analysisProgress) {
        Object.entries(dataset.analysisProgress).forEach(([analysisId, progress]) => {
          acc[analysisId] = { 
            ...progress, 
            dataId: dataset.id,
            type: 'analysis' 
          };
        });
      }
      return acc;
    }, {})
  };
  
  // Try to find the operationId across all operation types
  let progress = null;
  let operationType = null;
  
  // Check for data operations first (most common)
  if (dataStore[operationId]) {
    progress = dataStore[operationId].progress || { phase: 'unknown', percent: 0 };
    operationType = 'data';
  }
  
  // Check visualization operations
  if (!progress) {
    for (const dataset of Object.values(dataStore)) {
      if (dataset.vizProgress && dataset.vizProgress[operationId]) {
        progress = dataset.vizProgress[operationId];
        operationType = 'visualization';
        break;
      }
    }
  }
  
  // Check analysis operations
  if (!progress) {
    for (const dataset of Object.values(dataStore)) {
      if (dataset.analysisProgress && dataset.analysisProgress[operationId]) {
        progress = dataset.analysisProgress[operationId];
        operationType = 'analysis';
        break;
      }
    }
  }
  
  if (!progress) {
    return res.status(404).json({ error: 'Operation not found', operationId });
  }
  
  // Return progress info with operation type
  res.json({
    operationId,
    type: operationType,
    progress
  });
});

/**
 * Get all active operations progress
 * GET /api/progress
 */
router.get('/progress', (req, res) => {
  // Collect all active operations
  const activeOperations = {
    data: [],
    visualizations: [],
    analysis: []
  };
  
  // Collect data processing operations
  Object.values(dataStore).forEach(dataset => {
    // Only include operations that are actively processing
    if (dataset.status === 'processing' && dataset.progress) {
      activeOperations.data.push({
        id: dataset.id,
        name: dataset.originalName,
        progress: dataset.progress,
        startedAt: dataset.uploadDate
      });
    }
    
    // Collect visualization operations
    if (dataset.vizProgress) {
      Object.entries(dataset.vizProgress).forEach(([vizId, progress]) => {
        // Only include operations that are not complete
        if (progress.phase !== 'complete' && progress.phase !== 'error') {
          activeOperations.visualizations.push({
            id: vizId,
            dataId: dataset.id,
            dataName: dataset.originalName,
            progress,
            type: progress.vizType || 'unknown'
          });
        }
      });
    }
    
    // Collect analysis operations
    if (dataset.analysisProgress) {
      Object.entries(dataset.analysisProgress).forEach(([analysisId, progress]) => {
        // Only include operations that are not complete
        if (progress.phase !== 'complete' && progress.phase !== 'error') {
          activeOperations.analysis.push({
            id: analysisId,
            dataId: dataset.id,
            dataName: dataset.originalName,
            progress
          });
        }
      });
    }
  });
  
  // Calculate counts
  const counts = {
    data: activeOperations.data.length,
    visualizations: activeOperations.visualizations.length,
    analysis: activeOperations.analysis.length,
    total: activeOperations.data.length + 
           activeOperations.visualizations.length + 
           activeOperations.analysis.length
  };
  
  res.json({
    counts,
    activeOperations
  });
});

/**
 * Generate visualization
 * POST /api/visualize
 */
router.post('/visualize', express.json(), (req, res) => {
  const { dataId, options } = req.body;

  if (!dataId) {
    return res.status(400).json({ error: 'Data ID is required' });
  }

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (dataStore[dataId].status !== 'ready') {
    return res.status(400).json({
      error: 'Data is not ready for visualization',
      status: dataStore[dataId].status
    });
  }

  // Generate visualization specs
  const visualizationWorker = new Worker(path.join(__dirname, '../workers/visualization-generator.js'), {
    workerData: {
      dataId,
      filePath: dataStore[dataId].filePath,
      sampleData: dataStore[dataId].sampleData,
      options: options || {}
    }
  });

  visualizationWorker.on('message', (vizSpec) => {
    // Handle error object format with rich error information
    if (vizSpec.error) {
      // Log detailed error for debugging
      console.error('Visualization generation error:', {
        errorType: vizSpec.errorType || 'UNKNOWN_ERROR',
        error: vizSpec.error,
        details: vizSpec.details || null
      });
      
      // Store error information in the datastore for future reference
      if (!dataStore[dataId].vizErrors) {
        dataStore[dataId].vizErrors = [];
      }
      
      const errorId = uuidv4();
      dataStore[dataId].vizErrors.push({
        id: errorId,
        error: vizSpec.error,
        errorType: vizSpec.errorType || 'UNKNOWN_ERROR',
        timestamp: new Date().toISOString(),
        recoverable: vizSpec.recoverable === true,
        details: vizSpec.details || null
      });
      
      // If the error has partial results that can be shown, include them
      if (vizSpec.partialResults) {
        return res.status(207).json({ 
          warning: vizSpec.error,
          errorType: vizSpec.errorType,
          errorId,
          partialResults: vizSpec.partialResults,
          recoverable: true
        });
      }
      
      // Return appropriate status code based on error type
      const statusCode = vizSpec.recoverable === true ? 422 : 500;
      return res.status(statusCode).json({ 
        error: vizSpec.error,
        errorType: vizSpec.errorType || 'UNKNOWN_ERROR',
        errorId,
        errorDetails: vizSpec.details || null,
        recoverable: vizSpec.recoverable === true
      });
    }
    
    // Handle successful generation with warnings
    if (vizSpec.warnings && Array.isArray(vizSpec.warnings) && vizSpec.warnings.length > 0) {
      // Store the visualizations with warnings
      if (!dataStore[dataId].visualizations) {
        dataStore[dataId].visualizations = [];
      }
      
      // Store visualization data
      const vizId = uuidv4();
      dataStore[dataId].visualizations.push({
        id: vizId,
        spec: vizSpec.visualizations || vizSpec, // Handle both formats
        createdAt: new Date().toISOString(),
        hasWarnings: true,
        warnings: vizSpec.warnings
      });
      
      // Return with warnings
      return res.status(200).json({
        success: true,
        dataId,
        vizId,
        spec: vizSpec.visualizations || vizSpec,
        warnings: vizSpec.warnings,
        message: "Visualization generated with warnings"
      });
    }
    
    // Normal successful case
    if (!dataStore[dataId].visualizations) {
      dataStore[dataId].visualizations = [];
    }

    const vizId = uuidv4();
    dataStore[dataId].visualizations.push({
      id: vizId,
      spec: vizSpec,
      createdAt: new Date().toISOString()
    });

    res.json({
      success: true,
      dataId,
      vizId,
      spec: vizSpec
    });
  });

  visualizationWorker.on('error', (err) => {
    console.error('Worker thread error during visualization generation:', err);
    
    // Store error for debugging
    if (!dataStore[dataId].vizErrors) {
      dataStore[dataId].vizErrors = [];
    }
    
    const errorId = uuidv4();
    dataStore[dataId].vizErrors.push({
      id: errorId,
      error: err.message,
      errorType: 'WORKER_ERROR',
      timestamp: new Date().toISOString(),
      stack: err.stack
    });
    
    res.status(500).json({ 
      error: err.message, 
      errorType: 'WORKER_ERROR',
      errorId,
      message: 'An error occurred during visualization generation'
    });
  });
});

/**
 * Get a specific visualization
 * GET /api/visualize/:dataId/:vizId
 */
router.get('/visualize/:dataId/:vizId', (req, res) => {
  const { dataId, vizId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (!dataStore[dataId].visualizations) {
    return res.status(404).json({ error: 'No visualizations available' });
  }

  const viz = dataStore[dataId].visualizations.find(v => v.id === vizId);

  if (!viz) {
    return res.status(404).json({ error: 'Visualization not found' });
  }

  res.json(viz);
});

/**
 * Get all visualizations for a dataset
 * GET /api/visualize/:dataId
 */
router.get('/visualize/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  const visualizations = dataStore[dataId].visualizations || [];

  res.json({
    dataId,
    count: visualizations.length,
    visualizations: visualizations.map(v => ({
      id: v.id,
      type: v.spec.type,
      title: v.spec.config.title,
      createdAt: v.createdAt
    }))
  });
});

/**
 * Generate analytics and metadata
 * GET /api/analyze/:dataId
 */
router.get('/analyze/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (dataStore[dataId].status !== 'ready') {
    return res.status(400).json({
      error: 'Data is not ready for analysis',
      status: dataStore[dataId].status
    });
  }

  // Check if we already have analysis or if it's already in progress
  if (dataStore[dataId].analysis) {
    // Use cache if available
    const cachedAnalysis = cacheManager.get('analysis', dataId);
    if (cachedAnalysis) {
      return res.json(cachedAnalysis);
    }
    
    // Cache the analysis for future use
    cacheManager.set('analysis', dataId, dataStore[dataId].analysis);
    return res.json(dataStore[dataId].analysis);
  }

  // Check if analysis is already in progress
  if (dataStore[dataId].analysisInProgress) {
    return res.status(202).json({
      message: 'Analysis is already in progress',
      dataId,
      status: 'processing',
      progress: dataStore[dataId].analysisProgress || { phase: 'initializing', percent: 0 }
    });
  }

  // Mark as in progress to prevent duplicate requests
  dataStore[dataId].analysisInProgress = true;
  dataStore[dataId].analysisProgress = { 
    phase: 'initializing', 
    percent: 0,
    startedAt: Date.now()
  };

  // Generate unique operation ID for tracking
  const operationId = `analysis_${dataId}_${Date.now()}`;
  dataStore[dataId].currentAnalysisId = operationId;

  // Create a boolean flag to track if response has been sent
  let responseHandled = false;

  // Function to safely send response only once
  const safeSendResponse = (statusCode, data) => {
    if (!responseHandled) {
      responseHandled = true;
      res.status(statusCode).json(data);
    }
  };

  // Generate analytics in worker thread
  const analyticsWorker = new Worker(path.join(__dirname, '../workers/analytics.js'), {
    workerData: {
      dataId,
      operationId,
      filePath: dataStore[dataId].filePath,
      summary: dataStore[dataId].summary,
      dataTypes: dataStore[dataId].dataTypes
    }
  });

  // Handle progress updates
  analyticsWorker.on('message', (result) => {
    if (result.type === 'progress') {
      // Update progress information
      const { phase, progress, message } = result;
      
      // Store progress information
      dataStore[dataId].analysisProgress = {
        phase,
        percent: progress || 0,
        message,
        updatedAt: Date.now()
      };
      
      // Don't send response for progress updates
      return;
    } else if (result.type === 'complete') {
      // Store the analysis when complete
      delete dataStore[dataId].analysisInProgress;
      
      // Update progress to complete
      dataStore[dataId].analysisProgress = {
        phase: 'complete',
        percent: 100,
        message: 'Analysis completed',
        completedAt: Date.now()
      };
      
      // Store the final analysis
      dataStore[dataId].analysis = {
        ...result,
        generatedAt: new Date().toISOString()
      };

      // Cache the analysis for future use
      cacheManager.set('analysis', dataId, dataStore[dataId].analysis);

      // Send response with the analysis
      safeSendResponse(200, dataStore[dataId].analysis);
    } else if (result.type === 'error' || result.error) {
      // Handle error
      delete dataStore[dataId].analysisInProgress;
      
      // Update progress to error state
      dataStore[dataId].analysisProgress = {
        phase: 'error',
        percent: 0,
        message: `Error: ${result.error || 'Unknown error'}`,
        errorAt: Date.now()
      };
      
      // Store error information
      dataStore[dataId].analysisError = {
        message: result.error || 'Unknown analysis error',
        stack: result.stack,
        timestamp: new Date().toISOString()
      };
      
      // Send error response
      safeSendResponse(500, { 
        error: result.error || 'Analysis failed',
        dataId
      });
    }
  });

  // Handle worker errors
  analyticsWorker.on('error', (err) => {
    console.error(`Analysis worker error for ${dataId}:`, err);
    
    // Mark as no longer in progress
    delete dataStore[dataId].analysisInProgress;
    
    // Update progress to error state
    dataStore[dataId].analysisProgress = {
      phase: 'error',
      percent: 0,
      message: `Error: ${err.message}`,
      errorAt: Date.now()
    };
    
    // Store error information
    dataStore[dataId].analysisError = {
      message: err.message,
      stack: err.stack,
      timestamp: new Date().toISOString()
    };
    
    // Send error response
    safeSendResponse(500, { 
      error: err.message || 'Analysis worker failed',
      dataId
    });
  });

  // Start with an initial 202 Accepted response since analysis will continue
  safeSendResponse(202, {
    message: 'Analysis started',
    dataId,
    status: 'processing',
    operationId,
    progress: dataStore[dataId].analysisProgress || { phase: 'initializing', percent: 0 }
  });
});

/**
 * Get available datasets
 * GET /api/datasets
 */
router.get('/datasets', (req, res) => {
  const datasets = Object.values(dataStore).map(dataset => ({
    id: dataset.id,
    name: dataset.originalName,
    status: dataset.status,
    uploadDate: dataset.uploadDate,
    rowCount: dataset.summary ? dataset.summary.rowCount : null,
    columnCount: dataset.summary ? dataset.summary.columns.length : null,
    visualizationCount: dataset.visualizations ? dataset.visualizations.length : 0,
    hasAnalysis: Boolean(dataset.analysis)
  }));

  res.json(datasets);
});

/**
 * Delete a dataset
 * DELETE /api/datasets/:dataId
 */
router.delete('/datasets/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  // Delete the file
  const filePath = dataStore[dataId].filePath;
  if (fs.existsSync(filePath)) {
    fs.unlinkSync(filePath);
  }

  // Remove from data store
  delete dataStore[dataId];

  res.json({ success: true });
});

/**
 * Get recommendations for visualizations
 * GET /api/recommend/:dataId
 */
router.get('/recommend/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (dataStore[dataId].status !== 'ready') {
    return res.status(400).json({
      error: 'Data is not ready',
      status: dataStore[dataId].status
    });
  }

  try {
    // Import here to avoid circular dependencies
    const { recommendVisualizations } = require('../visualization/chart-selector');

    // Use cached sample data if available
    const sampleDataEntry = cacheManager.get('sampleData', dataId);
    const sampleData = sampleDataEntry || dataStore[dataId].sampleData;

    if (!sampleData || !Array.isArray(sampleData) || sampleData.length === 0) {
      return res.status(400).json({
        error: 'No sample data available for recommendations',
        dataId
      });
    }

    // Make sure we have the dataTypes property
    if (!dataStore[dataId].dataTypes) {
      // Initialize dataTypes if not present
      dataStore[dataId].dataTypes = {};
    }

    // Generate recommendations with safer options
    const recommendations = recommendVisualizations(
      sampleData,
      {
        dataTypes: dataStore[dataId].dataTypes || {},
        safeMode: true // Add a safe mode flag to avoid errors
      }
    );

    // Store in cache if not already stored
    if (!sampleDataEntry) {
      cacheManager.set('sampleData', dataId, sampleData);
    }

    res.json({
      dataId,
      recommendations
    });
  } catch (error) {
    console.error(`Error generating recommendations for dataset ${dataId}:`, error);
    res.status(500).json({
      error: 'Failed to generate recommendations',
      message: error.message,
      dataId
    });
  }
});

/**
 * Get optimal visualizations based on data analysis
 * GET /api/optimal-visualizations/:dataId
 */
router.get('/optimal-visualizations/:dataId', (req, res) => {
  const { dataId } = req.params;
  const limit = parseInt(req.query.limit) || 10;
  const progressive = req.query.progressive === 'true';

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (dataStore[dataId].status !== 'ready') {
    return res.status(400).json({
      error: 'Data is not ready for analysis',
      status: dataStore[dataId].status
    });
  }

  // Get any existing analysis to use as metadata
  const metadata = dataStore[dataId].analysis ? {
    dataTypes: dataStore[dataId].dataTypes,
    dimensions: dataStore[dataId].analysis.statistics ? 
      Object.keys(dataStore[dataId].analysis.statistics).filter(col => 
        dataStore[dataId].analysis.statistics[col].type === 'categorical'
      ) : null
  } : null;

  // For progressive loading
  if (progressive) {
    // Set up headers for SSE (Server-Sent Events)
    res.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive'
    });

    // Send initial status
    res.write(`data: ${JSON.stringify({ type: 'status', status: 'started', total: limit })}\n\n`);

    // Track how many visualizations we've sent
    let visualizationCount = 0;

    // Generate optimal visualizations using worker thread
    const visualizationWorker = new Worker(path.join(__dirname, '../workers/visualization-generator.js'), {
      workerData: {
        dataId,
        filePath: dataStore[dataId].filePath,
        sampleData: dataStore[dataId].sampleData,
        mode: 'optimal-progressive',
        options: {
          metadata,
          limit
        }
      }
    });

    visualizationWorker.on('message', (result) => {
      if (result.error) {
        res.write(`data: ${JSON.stringify({ type: 'error', error: result.error })}\n\n`);
        return res.end();
      }

      // Initialize visualizations array if needed
      if (!dataStore[dataId].visualizations) {
        dataStore[dataId].visualizations = [];
      }

      // Check if this is a single visualization or the final result
      if (result.type === 'visualization') {
        // This is a single visualization
        const vizId = uuidv4();
        
        // Store it
        dataStore[dataId].visualizations.push({
          id: vizId,
          spec: result.visualization,
          createdAt: new Date().toISOString(),
          isOptimal: true
        });

        // Send it to client
        res.write(`data: ${JSON.stringify({ 
          type: 'visualization', 
          vizId,
          spec: result.visualization,
          progress: ++visualizationCount,
          total: limit
        })}\n\n`);
      } else if (result.type === 'complete') {
        // Send completion event
        res.write(`data: ${JSON.stringify({ 
          type: 'complete', 
          count: visualizationCount,
          total: visualizationCount
        })}\n\n`);
        
        res.end();
      }
    });

    visualizationWorker.on('error', (err) => {
      res.write(`data: ${JSON.stringify({ 
        type: 'error', 
        error: err.message || 'Failed to generate optimal visualizations' 
      })}\n\n`);
      
      res.end();
    });

    // Handle client disconnect
    req.on('close', () => {
      try {
        if (visualizationWorker) {
          visualizationWorker.terminate();
        }
      } catch (err) {
        console.error('Error terminating worker:', err);
      }
    });
  } else {
    // Regular non-progressive mode
    // Generate optimal visualizations using worker thread
    const visualizationWorker = new Worker(path.join(__dirname, '../workers/visualization-generator.js'), {
      workerData: {
        dataId,
        filePath: dataStore[dataId].filePath,
        sampleData: dataStore[dataId].sampleData,
        mode: 'optimal',
        options: {
          metadata,
          limit
        }
      }
    });

    visualizationWorker.on('message', (result) => {
      // Handle structured error response
      if (result.error) {
        // Log detailed error for debugging
        console.error('Optimal visualization generation error:', {
          errorType: result.errorType || 'UNKNOWN_ERROR',
          error: result.error,
          details: result.details || null
        });
        
        // Store error information in the datastore for future reference
        if (!dataStore[dataId].vizErrors) {
          dataStore[dataId].vizErrors = [];
        }
        
        const errorId = uuidv4();
        dataStore[dataId].vizErrors.push({
          id: errorId,
          error: result.error,
          errorType: result.errorType || 'UNKNOWN_ERROR',
          timestamp: new Date().toISOString(),
          recoverable: result.recoverable === true,
          details: result.details || null
        });
        
        // If we have partial results
        if (result.partialResults && Array.isArray(result.partialResults) && result.partialResults.length > 0) {
          // Store the partial visualizations
          if (!dataStore[dataId].visualizations) {
            dataStore[dataId].visualizations = [];
          }
          
          // Add each partial visualization to storage
          const vizIds = [];
          result.partialResults.forEach(vizSpec => {
            const vizId = uuidv4();
            vizIds.push(vizId);
            dataStore[dataId].visualizations.push({
              id: vizId,
              spec: vizSpec,
              createdAt: new Date().toISOString(),
              isOptimal: true,
              isPartial: true
            });
          });
          
          // Return partial success with warning
          return res.status(207).json({
            dataId,
            warning: result.error,
            errorType: result.errorType || 'PARTIAL_GENERATION',
            errorId,
            count: result.partialResults.length,
            visualizations: result.partialResults,
            vizIds,
            recoverable: true,
            message: 'Partial visualizations generated with errors'
          });
        }
        
        // Return appropriate error status based on recoverability
        const statusCode = result.recoverable === true ? 422 : 500;
        return res.status(statusCode).json({
          error: result.error,
          errorType: result.errorType || 'UNKNOWN_ERROR',
          errorId,
          errorDetails: result.details || null,
          recoverable: result.recoverable === true
        });
      }
      
      // Handle successful generation with warnings
      if (result.warnings && Array.isArray(result.warnings) && result.warnings.length > 0) {
        // Store the visualizations with warnings
        if (!dataStore[dataId].visualizations) {
          dataStore[dataId].visualizations = [];
        }
        
        // Determine which visualizations to use
        const vizList = result.visualizations || [];
        const vizIds = [];
        
        // Add each visualization to storage with a unique ID
        vizList.forEach(vizSpec => {
          const vizId = uuidv4();
          vizIds.push(vizId);
          dataStore[dataId].visualizations.push({
            id: vizId,
            spec: vizSpec,
            createdAt: new Date().toISOString(),
            isOptimal: true,
            hasWarnings: true
          });
        });
        
        // Return success with warnings
        return res.status(200).json({
          success: true,
          dataId,
          count: vizList.length,
          visualizations: vizList,
          vizIds,
          warnings: result.warnings,
          message: 'Optimal visualizations generated with warnings'
        });
      }
      
      // Standard success case (no warnings)
      if (!dataStore[dataId].visualizations) {
        dataStore[dataId].visualizations = [];
      }

      // Determine which visualizations to use
      const vizList = result.visualizations || [];
      const vizIds = [];
      
      // Add each visualization to storage with a unique ID
      vizList.forEach(vizSpec => {
        const vizId = uuidv4();
        vizIds.push(vizId);
        dataStore[dataId].visualizations.push({
          id: vizId,
          spec: vizSpec,
          createdAt: new Date().toISOString(),
          isOptimal: true
        });
      });

      res.json({
        success: true,
        dataId,
        count: vizList.length,
        visualizations: vizList,
        vizIds
      });
    });

    visualizationWorker.on('error', (err) => {
      console.error('Worker thread error during optimal visualization generation:', err);
      
      // Store error for debugging
      if (!dataStore[dataId].vizErrors) {
        dataStore[dataId].vizErrors = [];
      }
      
      const errorId = uuidv4();
      dataStore[dataId].vizErrors.push({
        id: errorId,
        error: err.message || 'Unknown worker error',
        errorType: 'WORKER_ERROR',
        timestamp: new Date().toISOString(),
        stack: err.stack
      });
      
      res.status(500).json({ 
        error: err.message || 'Failed to generate optimal visualizations',
        errorType: 'WORKER_ERROR',
        errorId,
        message: 'An error occurred during optimal visualization generation'
      });
    });
  }
});

/**
 * Get diagnostic information for a dataset
 * GET /api/diagnostics/:dataId
 */
router.get('/diagnostics/:dataId', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  // Gather diagnostic information
  const diagnostics = {
    dataId,
    status: dataStore[dataId].status,
    fileName: dataStore[dataId].originalName,
    fileExists: dataStore[dataId].filePath ? fs.existsSync(dataStore[dataId].filePath) : false,
    uploadDate: dataStore[dataId].uploadDate,
    lastModified: dataStore[dataId].filePath ? 
      fs.existsSync(dataStore[dataId].filePath) ? 
        fs.statSync(dataStore[dataId].filePath).mtime.toISOString() : null 
      : null,
    fileSize: dataStore[dataId].filePath ? 
      fs.existsSync(dataStore[dataId].filePath) ? 
        fs.statSync(dataStore[dataId].filePath).size : null 
      : null,
    dataStoreSample: {
      exists: Boolean(dataStore[dataId].sampleData),
      length: dataStore[dataId].sampleData ? dataStore[dataId].sampleData.length : 0
    },
    cacheSample: {
      exists: Boolean(cacheManager.get('sampleData', dataId)),
      length: cacheManager.get('sampleData', dataId) ? 
        cacheManager.get('sampleData', dataId).length : 0
    },
    summary: dataStore[dataId].summary || null,
    progress: dataStore[dataId].progress || null,
    hasAnalysis: Boolean(dataStore[dataId].analysis),
    analysisProgress: dataStore[dataId].analysisProgress || null,
    vizCount: dataStore[dataId].visualizations ? dataStore[dataId].visualizations.length : 0,
    errors: {
      processingError: dataStore[dataId].error || null,
      analysisError: dataStore[dataId].analysisError || null,
      vizErrors: dataStore[dataId].vizErrors ? dataStore[dataId].vizErrors.length : 0
    }
  };

  res.json(diagnostics);
});

/**
 * Rebuild sample data for a dataset
 * POST /api/diagnostics/:dataId/rebuild-sample
 */
router.post('/diagnostics/:dataId/rebuild-sample', (req, res) => {
  const { dataId } = req.params;

  if (!dataStore[dataId]) {
    return res.status(404).json({ error: 'Data not found' });
  }

  if (!dataStore[dataId].filePath || !fs.existsSync(dataStore[dataId].filePath)) {
    return res.status(400).json({ 
      error: 'Original file not available for rebuilding sample',
      dataId,
      filePath: dataStore[dataId].filePath || null,
      fileExists: dataStore[dataId].filePath ? fs.existsSync(dataStore[dataId].filePath) : false
    });
  }

  try {
    // Import the memory-efficient processor
    const { MemoryEfficientCSVProcessor } = require('../data/csv-parser');
    
    // Create a new processor instance with progress reporting
    const processor = new MemoryEfficientCSVProcessor({
      sampleSize: 1000,
      batchSize: 5000,
      progressCallback: (progressData) => {
        // Store progress in dataStore
        if (!dataStore[dataId].rebuildProgress) {
          dataStore[dataId].rebuildProgress = {};
        }
        dataStore[dataId].rebuildProgress = {
          ...progressData,
          updatedAt: Date.now()
        };
      }
    });
    
    // Start processing asynchronously
    console.log(`Rebuilding sample data for dataset ${dataId}`);
    
    // Store the rebuild operation status
    dataStore[dataId].rebuildInProgress = true;
    dataStore[dataId].rebuildStartTime = Date.now();
    
    // Return immediate response that rebuild has started
    res.json({
      success: true,
      message: "Sample data rebuild started. Check rebuild status with diagnostics endpoint.",
      dataId
    });
    
    // Process the file
    processor.processFile(dataStore[dataId].filePath)
      .then(processedData => {
        // Store the regenerated sample data in the data store
        dataStore[dataId].sampleData = processedData.sampleData;
        
        // Store in cache for future requests
        if (processedData.sampleData.length > 0) {
          cacheManager.set('sampleData', dataId, processedData.sampleData, 30 * 60 * 1000); // 30 minute TTL
          console.log(`Regenerated and stored ${processedData.sampleData.length} sample items for dataset ${dataId}`);
        }
        
        // Update rebuild status
        dataStore[dataId].rebuildInProgress = false;
        dataStore[dataId].rebuildCompleted = true;
        dataStore[dataId].rebuildEndTime = Date.now();
        dataStore[dataId].rebuildDuration = Date.now() - dataStore[dataId].rebuildStartTime;
        dataStore[dataId].rebuildSuccess = true;
        dataStore[dataId].rebuildResult = {
          sampleSize: processedData.sampleData.length,
          processingTime: processedData.summary.processingTime
        };
      })
      .catch(err => {
        console.error(`Error rebuilding sample data for dataset ${dataId}:`, err);
        
        // Update rebuild status with error
        dataStore[dataId].rebuildInProgress = false;
        dataStore[dataId].rebuildCompleted = true;
        dataStore[dataId].rebuildEndTime = Date.now();
        dataStore[dataId].rebuildDuration = Date.now() - dataStore[dataId].rebuildStartTime;
        dataStore[dataId].rebuildSuccess = false;
        dataStore[dataId].rebuildError = {
          message: err.message,
          stack: err.stack
        };
      });
  } catch (error) {
    console.error(`Failed to start rebuild for dataset ${dataId}:`, error);
    res.status(500).json({
      success: false,
      error: error.message,
      dataId
    });
  }
});

/**
 * Get cache statistics
 * GET /api/cache/stats
 */
router.get('/cache/stats', (req, res) => {
  res.json({
    success: true,
    stats: cacheManager.getStats()
  });
});

/**
 * Clear a specific cache or all caches
 * POST /api/cache/clear/:cacheType?
 * Query Parameters:
 *  - dataId: Optional dataId to clear only cache entries for a specific dataset
 */
router.post('/cache/clear/:cacheType?', (req, res) => {
  const { cacheType } = req.params;
  const { dataId } = req.query;
  
  if (dataId) {
    // Clear cache entries for a specific dataset
    cacheManager.invalidateDataset(dataId);
    
    res.json({
      success: true,
      message: `Cache entries for dataset ${dataId} cleared successfully`
    });
    return;
  }
  
  if (cacheType && dataCache[cacheType]) {
    // Clear specific cache type
    dataCache[cacheType].clear();
    
    // Reset hit stats
    dataCache.hits[cacheType] = 0;
    dataCache.misses[cacheType] = 0;
    
    // Adjust total size (this is approximate since we're clearing all at once)
    cacheManager.totalSize = 0;
    
    res.json({
      success: true,
      message: `Cache '${cacheType}' cleared successfully`
    });
  } else if (!cacheType) {
    // Clear all caches
    Object.keys(dataCache).forEach(key => {
      if (key !== 'config' && key !== 'lastCleanup' && 
          key !== 'hits' && key !== 'misses' && 
          typeof dataCache[key].clear === 'function') {
        dataCache[key].clear();
      }
    });
    
    // Reset hit stats
    Object.keys(dataCache.hits).forEach(key => {
      dataCache.hits[key] = 0;
      dataCache.misses[key] = 0;
    });
    
    // Reset total size
    cacheManager.totalSize = 0;
    
    res.json({
      success: true,
      message: 'All caches cleared successfully'
    });
  } else {
    res.status(404).json({
      success: false,
      error: `Cache type '${cacheType}' not found`
    });
  }
});

module.exports = router;
